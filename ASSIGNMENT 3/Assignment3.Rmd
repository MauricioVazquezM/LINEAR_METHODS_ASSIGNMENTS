---
title: 'Tarea 4: Estadistica Aplicada II'
author: "Mauricio Vazquez (000191686)"
date: '2024-12-11'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, fig.width=10)

# Libraries
library(MASS)  
library(ggplot2)
library(GGally)
library(mclust)
library(kableExtra)
library(FactoMineR)
library(knitr)
library(caret)
library(dplyr)
library(gridExtra)
library(reshape2) 
library(corrplot)
library(ggcorrplot)
```

## Ejercicio 01

Una cadena de tiendas de ventas al menudeo da seguimiento a la productividad y costos de sus tiendas. Se te proporcionan datos de uno de sus centros de distribución recolectados durante un año. Cada observación representa el registro de la actividad semanal. 

Las variables acopiadas son:

- $X_1$: Número de paquetes distribuidos.
- $X_2$: Costos indirectos (en porcentaje) asociados al total de horas trabajadas.
- $X_3$: Variable cualitativa que codifica si la semana tuvo días feriados (1) o no (0).
- $Y$: Total de horas trabajadas.

1. Elabora los diagramas de caja y brazos de las variables explicativas. ¿Se pueden observar datos atípicos? ¿Existen espacios (gaps) sin observaciones?

```{r fig.align='center', fig.width=6, fig.height=4, echo=FALSE}
# Lectura de datos
datos_1 <- read.table(file = 'datos_prob_1.txt', 
                      sep = '\t', 
                      header =FALSE,
                      col.names = c('Y','X_1','X_2','X_3'))

# Plos de caja y brazos
c1 <- ggplot(data = datos_1) +
  aes(x = X_1) +
  geom_boxplot(mapping = aes(y = 'X_1')) +
  labs(title = "Paquetes distribuidos", x="Valor", y="") +
  theme_minimal()

c2 <- ggplot(data = datos_1) +
  aes(x = X_2) +
  geom_boxplot(mapping = aes(y = 'X_2')) +
  labs(title = "Costos indirectos", x="Valor", y="") +
  theme_minimal()

# Histogramas
h1 <-  ggplot(data = datos_1) +
  aes(x = X_1) +
  geom_histogram(bins = 50) +
  labs(title = "Paquetes distribuidos", x="Valor", y="Frecuencia") +
  theme_minimal()

h2 <- ggplot(data = datos_1) +
  aes(x = X_2) +
  geom_histogram(bins = 50) +
  labs(title = "Costos indirectos", x="Valor", y="Frecuencia") +
  theme_minimal()

# Conteo por catergoria
c3 <- ggplot(data = datos_1) +
  aes(x = factor(X_3)) +
  geom_bar() +
  labs(title = "Semana con dias feriados", x ="Días feriados (1) o no (0)", y="Conteo") +
  theme_minimal()

# Mostrar ambos gráficos en un solo layout
grid.arrange(c1, c2, h1, h2, c3, ncol = 2)

```

Como podemos observar en los boxplots, se pueden ver datos atípicos en el gráfico correspondiente a la variable $X_1$ (Paquetes distribuidos). Existen puntos fuera de los límites del rango intercuartílico (IQR), indicando valores inusualmente altos. Por su parte, en la variable $X_2$ (Costos indirectos), tambien se puede observar datos fuera del $IQR$, es decir, datos atípicos. 

Por otro lado, se puede analizar que, en fecto, hay espacios sin observaciones en ambas variables mencionadas previamente. Además, se puede observar que hay un desequilibrio en las frecuencias entre semanas con días feriados (1) y semanas sin días feriados (0).

<br>

2. Los datos son proporcionados en orden cronológico. ¿Qué se observa en las gráficas cronológicas?

```{r fig.align='center', fig.width=7, fig.height=5, echo=FALSE}
# Graficos de dispersion
d1 <- ggplot(data = datos_1) +
  aes(x = 1:52, y = X_1) +
  geom_point() +
  labs(title = "Paquetes distribuidos por semana", x="Semana", y="Valor") +
  theme_classic()

d2 <- ggplot(data = datos_1) +
  aes(x = 1:52, y = X_2) +
  geom_point() +
  labs(title = "Costos indirectos por semana", x="Semana", y="Valor") +
  theme_classic()

d3 <- ggplot(data = datos_1) +
  aes(x = 1:52, y = X_3) +
  geom_point() +
  labs(title = "Semana con dias feriados", x="Semana", y="Valor") +
  theme_classic()

# Mostrar gráficos en un solo layout
grid.arrange(d1, d2, d3 , ncol = 2)
```

  * Se observa una tendencia creciente a lo largo del tiempo. El número de paquetes distribuidos incrementa progresivamente, especialmente a partir de la semana 30. Esto podría indicar un aumento en la actividad operativa..
  * Los costos indirectos no muestran una tendencia clara, aunque parecen oscilar dentro de un rango constante (entre 5 y 9). Esto sugiere que los costos indirectos no están correlacionados directamente con el tiempo.
  * Los días feriados (valor 1) son pocos y están dispersos en el tiempo, mientras que la mayoría de las semanas no tienen días feriados (valor 0)

<br>

3. Obtén las gráficas de dispersión de los datos y la matriz de correlaciones. ¿Qué se puede deducir de estas gráficas y matriz?

```{r fig.align='center', fig.width=4, fig.height=4, echo=FALSE}
# Dataset reducido
datos_1$X_3 <- as.factor(datos_1$X_3)
numerical_vars <- datos_1[, c('Y','X_1','X_2')]

# Creando el pairs plot 
ggpairs(
  numerical_vars,
  lower = list(continuous = "points"),
  diag = list(continuous = "densityDiag"), 
  upper = list(continuous = "cor"),    
  aes(color = datos_1$X_3, alpha = 0.7) 
) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"), 
    axis.title = element_text(size = 12),               
    axis.text = element_text(size = 10), 
    axis.text.x = element_text(angle = 90, hjust = 1),
    axis.text.y = element_text(angle = 0), 
    legend.position = "top"                             
  ) +
  labs(title = "Analisis de correlación (por semana)", color = "Semana con dia feriado o no")
```

- Las semanas con días feriados ($X_3 = 1$) parecen tener relaciones más fuertes entre las variables, especialmente entre $Y$ y $X_1$ y entre $Y$ y $X_2$.
- Las distribuciones de las variables cambian significativamente dependiendo de si hubo días feriados.

<br>

4. Estima el modelo de regresión lineal. Proporciona la ecuación de regresión estimada. ¿Cómo se interpretan en este contexto los coeficientes de regresión?

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}
# Lectura de datos
datos_1 <- read.table(file = 'datos_prob_1.txt', 
                      sep = '\t', 
                      header =FALSE,
                      col.names = c('Y','X_1','X_2','X_3'))

# Estimar el modelo de regresión lineal
modelo_1 <- lm(Y ~ X_1 + X_2 + X_3, data = datos_1)
coeficientes <- coef(modelo_1)

# Intercepto
intercepto <- coeficientes["(Intercept)"]

# Coeficiente de X_1
coef_X1 <- coeficientes["X_1"]

# Coeficiente de X_2
coef_X2 <- coeficientes["X_2"]

# Coeficiente de X_3
coef_X3 <- coeficientes["X_3"]
```

La ecuación estimada es:

\[
\hat{Y} = `r round(intercepto, 2)` + `r round(coef_X1, 3)` X_1 + `r round(coef_X2, 4)` X_2 + `r round(coef_X3, 2)` X_3
\]

* $\beta_0 = `r round(intercepto, 3)`$: Este es el intercepto, que representa las horas trabajadas cuando $X_1$, $X_2$, y $X_3$ son 0.
* $\beta_1 = `r round(coef_X1, 3)`$: Por cada paquete adicional distribuido ($X_1$), se espera que $Y$ (horas trabajadas) aumente en promedio `r round(coef_X1, 3)` horas, manteniendo constantes $X_2$ y $X_3$.
* $\beta_2 = `r round(coef_X2, 4)`$: Por cada incremento del 1% en los costos indirectos ($X_2$), se espera que $Y$ aumente en promedio `r round(coef_X2, 3)` horas, manteniendo constantes $X_1$ y $X_3$.
* $\beta_3 = `r round(coef_X3, 3)`$: Durante semanas con días feriados ($X_3 = 1$), se espera que $Y$ aumente en promedio `r round(coef_X3, 2)` horas en comparación con semanas sin días feriados ($X_3 = 0$), manteniendo constantes $X_1$ y $X_2$.

<br>

5. Obtén los residuales y elabora un diagrama de caja y brazos de los residuales. ¿Qué se puede inferir del diagrama?

```{r fig.align='center', fig.width=2.5, fig.height=2.5, echo=FALSE}
# Calcular los residuales
residuales <- residuals(modelo_1)

# Df auxiliar
residuales_df <- data.frame(Residuales = residuales)

# Diagrama de caja y brazos
ggplot(residuales_df, aes(y = Residuales)) +
  geom_boxplot(color = "black") +
  labs(
    title = "Boxplot de residuales",
    y = "Residuales"
  ) +
  theme_minimal()
```

En primer lugar, se puede observar que los residuales parecen estar centrados en torno a 0, lo que sugiere que el modelo no tiene un sesgo, al menos de forma evidente. En segundo lugar, la caja y los brazos son aproximadamente simétricos, indicando que la distribución de los residuales es cercana a una distribución normal. Por último, podemos analizar que el rango intercuartílico, es razonablemente amplio. Esto indica una variación moderada en los residuales.

<br>

6. Grafica los residuales contra $\hat{Y}$, $X_1$, $X_2$, $X_3$ y $X_1 X_2$. Elabora también un QQ plot. Proporciona una interpretación.

```{r fig.align='center', fig.width=8, fig.height=5, echo=FALSE}
# Residuales y valores ajustados
residuales <- residuals(modelo_1)
valores_ajustados <- fitted(modelo_1)

# variable X_1 * X_2
datos_1$X1_X2 <- datos_1$X_1 * datos_1$X_2

# DF auxiliar
residuals_df <- data.frame(
  Residuales = residuales,
  ValoresAjustados = valores_ajustados,
  X_1 = datos_1$X_1,
  X_2 = datos_1$X_2,
  X_3 = datos_1$X_3,
  X1_X2 = datos_1$X1_X2
)

# Plot residuales vs valores ajustados
plot1 <- ggplot(residuals_df, aes(x = Residuales, y = ValoresAjustados)) +
  geom_point(alpha = 0.7) +
  labs(title = "vs. Valores Ajustados", x = "Residuales", y = "Y_hat") +
  theme_minimal()

# Plot residuales vs X_1
plot2 <- ggplot(residuals_df, aes(x = Residuales, y = X_1)) +
  geom_point(alpha = 0.7) +
  labs(title = "vs. X_1", x = "Residuales", y = "X_1") +
  theme_minimal()

# Plot residuales vs X_2
plot3 <- ggplot(residuals_df, aes(x = Residuales, y = X_2)) +
  geom_point(alpha = 0.7) +
  labs(title = "vs. X_2", x = "Residuales", y = "X_2") +
  theme_minimal()

# Plot residuales vs X_3
plot4 <- ggplot(residuals_df, aes(x = Residuales, y = X_3)) +
  geom_point(alpha = 0.7) +
  labs(title = "vs. X_3", x = "Residuales", y = "X_3") +
  theme_minimal()

# Plot residuales vs X_1 * X_2
plot5 <- ggplot(residuals_df, aes(x = Residuales, y = X1_X2)) +
  geom_point(alpha = 0.7) +
  labs(title = "vs. X_1 * X_2", x = "Residuales", y = "X_1 * X_2") +
  theme_minimal()

# QQ Plot
plot6 <- ggplot(residuals_df, aes(sample = Residuales)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "QQ Plot Residuales", x = "Cuantiles Teóricos", y = "Cuantiles Residuales") +
  theme_minimal()

# Grid
grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, ncol = 3)

```

* Residuales vs $\hat{Y}$: los residuales parecen dispersos de manera aleatoria en torno a 0.
* Residuales vs $X_1$: los residuales están razonablemente dispersos alrededor de 0.
* Residuales vs $X_2$: los residuales están razonablemente dispersos alrededor de 0, similar a en $X_1$. 
* Residuales vs. $X_3$: los residuales están distribuidos simétricamente en torno a 0 para ambas categorías de $X_3$ (0 y 1).
* Residuales vs. $X_1 \cdot X_2$: los residuales están razonablemente dispersos alrededor de 0, como en $X_1$.
* QQ-plot: aunque los residuales son cercanos a una distribución normal, podría haber ligeras desviaciones en los valores extremos.

<br>

7. Grafica los residuales en el tiempo. ¿Se observa evidencia de correlación en los residuales?

```{r fig.align='center', fig.width=5, fig.height=2.5, echo=FALSE}
# Calcular los residuales
residuales <- residuals(modelo_1)

# Df auxiliar
residuales_df <- data.frame(Residuales = residuales)

# Scatterplot
ggplot(residuales_df, aes(x = 1:52, y = Residuales)) +
  geom_point(color = "black") +
  labs(
    title = "Residuales en el tiempo",
    x= 'Semana',
    y = "Residuales"
  ) +
  theme_minimal()
```

Al analizar el plot obtenido, los residuales parecen estar distribuidos de manera aleatoria alrededor de 0 a lo largo de las semanas. Además, no se observa un patrón claro de tendencia creciente, decreciente o cíclica en los residuales. Por último, a manera de observación, la falta de un patrón sistemático en el gráfico sugiere que no hay evidencia significativa de correlación en los residuales.

<br>

8. Realiza una prueba de Brown-Forsythe usando un $\alpha=0.01$. Expresa la regla de decisión y tu conclusión.

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}
# Residuales
datos_1$residual <- modelo_1$residuals

# Dividiendo grupo por la mediana
grupo.1 <- datos_1[which(modelo_1$fitted.values <= median(modelo_1$fitted.values)),]
grupo.2 <- datos_1[which(modelo_1$fitted.values > median(modelo_1$fitted.values)),]

# Calculando medianas de cada grupo
mediana.epsilon.gorro.1 <- median(grupo.1$residual)
mediana.epsilon.gorro.2 <- median(grupo.2$residual)

# Calculando los valores absolutos de las desviaciones respecto a la mediana
grupo.1$d <- abs(grupo.1$residual-mediana.epsilon.gorro.1)
grupo.2$d <- abs(grupo.2$residual-mediana.epsilon.gorro.2)

# Calculando las medias de las desviaciones absolutas en cada grupo
d.media.1 <- mean(grupo.1$d)
d.media.2 <- mean(grupo.2$d)

# Suma de cuadrados
SSD <-(sum((grupo.1$d - d.media.1)^2) + sum((grupo.2$d - d.media.2)^2)) / (nrow(datos_1) - 2)

# Calculando el estadisticod de prueba
t.star.bf <- abs(d.media.1 - d.media.2)/sqrt(SSD*(1/nrow(grupo.1) + 1/nrow(grupo.2)))

# Calculando el valor p
t.star.bf.p.value <- pt(q = t.star.bf, df = nrow(datos_1)-2, lower.tail = FALSE)
```

Dado que, bajo la hipótesis de varianzas constantes, la estadística de prueba sigue una distribución t, y considerando que el valor calculado de la estadística de prueba es $`r round(t.star.bf, 4)`$ y su correspondiente p-valor es $`r round(t.star.bf.p.value, 5)`$, el cual es menor que el nivel de significancia $\alpha = 0.01$, se rechaza la hipótesis nula. Esto nos permite concluir que existe evidencia suficiente para afirmar que las varianzas de los grupos no son homogéneas, lo que indica la presencia de heterocedasticidad en los datos.

<br>

## Ejercicio 02

La masa muscular de una persona se espera que disminuya con la edad. Para explorar esta relación en mujeres, una nutrióloga selecciona aleatoriamente a 15 mujeres de cada grupo de edades por décadas, comenzado en la edad 40 y terminando en la edad 79. $X$ es la edad y $Y$ es la medida de masa muscular. Supón que el modelo de regresión lineal simple es apropiado.

1. Obtén la función de regresión estimada. Grafica la función de regresión y los datos. ¿Te parece que la regresión lineal tenga un buen ajuste? ¿El gráfico apoya la hipótesis de que la masa muscular decrece con la edad?

```{r fig.align='center', fig.width=4, fig.height=3, echo=FALSE}
# Lectura de datos
datos_2 <-read.table(file = 'datos_prob_2.txt', 
                   sep = '|', 
                   header = FALSE, 
                   col.names = c('Masa', 'Edad'))



# Modelo de regresion multiple
modelo_2 <- lm(data = datos_2, formula = Masa ~ Edad)

# Summariy del modelo
summary(modelo_2)

# Gráfico de regresión
ggplot(data = datos_2) + 
  aes(x = Edad, y = Masa) +
  geom_point() +
  geom_smooth(method = 'lm', formula = y ~ x, se = TRUE) +
  theme_bw() +
  labs(title = "Gráfico de Regresión")

# Coeficientes
coeficientes <- coef(modelo_2)
intercepto <- coeficientes["(Intercept)"]
coef_edad <- coeficientes["Edad"]

# R-cuadrada y valor-p
pvalor <- summary(modelo_2)$fstatistic[1]  
gl <- summary(modelo_2)$fstatistic[2:3]
p_valor <- pf(q = pvalor, df1 = gl[1], df2 = gl[2], lower.tail = FALSE)
r2 <- summary(modelo_2)$r.squared
```

La ecuación estimada es:

\[
\hat{Y} = `r round(intercepto, 2)` + `r round(coef_edad, 2)` X_{Edad}
\]

Desde mi perspectiva, la regresión lineal tiene un buen ajuste, y el plot apoya la hipótesis de que la masa muscular disminuye con la edad. Esto se confirma tanto por los indicadores estadísticos ($R^2= `r round(r2, 4)`$ y $p_{value}= `r round(p_valor, 7)`$, ambos significativo) como por el plot de los datos.

<br>
	
2. Obtén lo siguiente: 1) un estimador puntual de la diferencia en la media de la masa muscular para las mujeres con diferencias de 1 año de edad; 2) un estimador puntual de la media de la masa muscular para mujeres de 60 años de edad; 3) el valor del residual para el 8avo. caso; 4) un estimador puntual de $\sigma^2$.

\begin{align*}
E[Y_{k+1} - Y_k] &= E[\beta_0 + \beta_1 X_k + \epsilon_k - \beta_0 - \beta_1 X_{k-1} - \epsilon_{k-1}] \\
&= E[\beta_1 X_k + \epsilon_k - \beta_1 X_{k-1} - \epsilon_{k-1}] \\
&= E[\beta_1 (X_k - X_{k-1}) + \epsilon_k - \epsilon_{k-1}] \\
&= E[\beta_1] + E[\epsilon_k - \epsilon_{k-1}] \\
&= \beta_1
\end{align*}

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}
# para 60 años
y.gorro.60 <- modelo_2$coefficients[1] + modelo_2$coefficients[2]*60

# Residual del 8vo caso
residuales <- residuals(modelo_2)
residual8 <- residuales[8]

# Estimador puntual sigma**2
sigma.2.gorro <- sum(modelo_2$residuals^2)/(nrow(datos_2)-2)
```

* Un estimador puntual de la media de la masa muscular para mujeres de 60 años de edad: $\hat{Y_{60}} = `r round(y.gorro.60, 4)`$.

* Valor del residual para el 8avo. caso: $Residual = `r round(residual8, 4)`$.

* Estimador puntual de $\sigma^2 = `r round(sigma.2.gorro, 4)`$.

<br>

3. Contruye un Q-Q plot de los residuales. Calcula el coeficiente de correlación entre los residuales ordenados y sus valores esperados conforme a una distribución normal para determinar si el supuesto de normalidad se sostiene en este caso. Utiliza la tabla B.6 y un $\alpha=0.1$. ¿Cuál es tu conclusión?

```{r fig.align='center', fig.width=4, fig.height=4, echo=FALSE}
# Calcular los residuales
residuales <- residuals(modelo_2)

# Df auxiliar
residuales_df <- data.frame(Residuales = residuales)

# QQ Plot
ggplot(residuals_df, aes(sample = Residuales)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "QQ Plot Residuales", x = "Cuantiles Teóricos", y = "Cuantiles Residuales") +
  theme_minimal()

# Supuesto de normalidad
ranking <- rank(modelo_2$residuals, ties.method = 'first')
valor.esperado <- sqrt(59) * sd(modelo_2$residuals) / sqrt(58) * qnorm(p = (ranking - 0.375)/(60 + 0.25))
correlacion <- cor(x = modelo_2$residuals, y = valor.esperado)
```

Para un total de 60 observaciones, según la tabla de valores críticos de Looney y Gulledge (1985), el valor crítico correspondiente a un nivel de significancia $\alpha = 0.10$ es 0.984. Recordar que la hipótesis nula de esta prueba establece que los datos siguen una distribución normal. Dado que la correlación observada supera el valor crítio, no se encuentra evidencia suficiente para rechazar la hipótesis nula. Por lo tanto, se confirma que los datos presentan normalidad.

\newpage

## Ejercicio 03

Se llevó a cabo un estudio de la relación que existe entre la cantidad de grasa corporal (variable respuesta) y el grosor del pliegue del triceps, la circunferencia del muslo y la circunferencia del antebrazo (variables explicativas). Se realizaron mediciones en 20 mujeres saludables de entre 25 y 34 años de edad. Dado que el procedimiento de medición de la cantidad de grasa corporal es complejo y sujeto a error, se desea ajustar un modelo de regresión lineal múltiple para poder realizar estimaciones futuras.

1. Ajusta el modelo de regresión lineal múltiple correspondiente.

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}
# Lectura de datos
datos_3 <-read.table(file = 'datos_prob_3.txt', 
                   sep = '|', 
                   header = TRUE, 
                   encoding = 'utf-8')

#Ajuste de un modelo de regresion lineal multiple
modelo_3 <- lm(data = datos_3, formula = Fat ~ Triceps + Thigh + Midarm)
summary(modelo_3)
```

<br>

2. Ajusta un modelo de regresión lineal simple para cada una de las variables explicativas. Obtén el $SSR$ de cada una de las regresiones.

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}
# Modelo triceps
modelo.3.1 <- lm(data = datos_3, formula = Fat ~ Triceps)
anova.3.1 <- anova(object = modelo.3.1)
a <- anova.3.1$`Sum Sq`[1]

# Modelo muslo
modelo.3.2 <- lm(data = datos_3, formula = Fat ~ Thigh)
anova.3.2 <- anova(object = modelo.3.2)
b <- anova.3.2$`Sum Sq`[1]

# Modelo antebrazo
modelo.3.3 <- lm(data = datos_3, formula = Fat ~ Midarm)
anova.3.3 <- anova(object = modelo.3.3)
c<- anova.3.3$`Sum Sq`[1]
```

Como resultado de las tres diferentes regresiones imples, tenemos los siguientes $SSR$'s:

* $SSR_{Triceps} = `r round(a, 4)`$
* $SSR_{Muslo} = `r round(b, 4)`$
* $SSR_{Antebrazo} = `r round(c, 4)`$

<br>

3. Calcula $SSR_{Triceps,Thigh}$.

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}
# Modelo multiple con tricep y muslo
model_t_m <- lm(data = datos_3, formula = Fat ~ Triceps + Thigh)
anova_t_m <- anova(object = model_t_m)
t_m <- sum(anova_t_m$`Sum Sq`[1:2])
```

Respuesta: $SSR_{Triceps,Thigh} = `r round(t_m, 4)`$.

<br>

4. Calcula $SSR_{Thigh | Triceps}$.

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}
# Modelo multiple thigh given triceps
t_m2 <- anova_t_m$`Sum Sq`[2]
```

Respuesta: $SSR_{Thigh | Triceps} = `r round(t_m2, 4)`$.

<br>

5. Calcula $SSR_{Midarm | Triceps,Thigh}$.

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}
modelo_m_tt <- lm(data = datos_3, formula = Fat ~ Triceps + Thigh + Midarm)
anova_m_tt <- anova(object = modelo_m_tt)
k <- sum(anova_m_tt$`Sum Sq`[3])
```

Respuesta: $SSR_{Midarm | Triceps,Thigh} = `r round(k, 4)`$.

<br>

6. Se desea verificar si la variable correspondiente al antebrazo se puede eliminar del análisis. Realiza un análisis $ANOVA$ y la prueba $F$ correspondiente y determina si existe evidencia estadística que permita eliminar a la variable.

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```

7. Se te pide ahora evaluar la posibilidad de eliminar tanto la variable correspondiente a las mediciones del antebrazo como la variable correspondiente a las mediciones del muslo. Realiza la prueba correspondiente, nuevamente considerando un $\alpha=0.01$ y argumenta tu conclusión.

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```

8. Completa la tabla que se muestra a continuación con los valores de los coeficientes de regresión estimados al incluir las variables señaladas en la primera columna. ¿Qué observas en los estimadores de los coeficientes?¿Son estables o inestables? ¿Qué significa que los coeficientes sean estables o inestables?

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```

9. Se desea ahora formalmente probar si las interacciones entre las variables deberían ser agregadas al modelo.

    a. Plantea el modelo completo con interacciones.
    
```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```
  
    b. Calcula la matriz de correlaciones del modelo con interacciones.
    
```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```
  
    c. Ajusta el modelo lineal con las variables de interacción. Considera las variables explicativas centradas (i.e., centra las variables explicativas y vuelve a calcular las interacciones). Describe brevemente los resultados del ajuste del modelo.
    
```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```
  
    d. ¿Cuál es la relación entre las variables principales y las interacciones? ¿Se refuerzan o se contrarrestan?
    
```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```
  
    e. Obtén las sumas de cuadrados de la regresión (SSR) marginales de las variables del modelo con interacciones.
    
```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```
  
    f. Realiza una prueba F para determinar si es apropiado retirar las variables de interacción del modelo. Plantea las hipótesis de la prueba, el criterio de descarte y los resultados de la prueba. Utiliza un nivel de prueba del 0.05.
    
```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```
  
10. Calcula la matriz de correlaciones lineales entre las variables explicativas. Interpreta tus resultados.

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```

11. Calcula los factores de inflación de varianzas. ¿Qué puedes concluir respecto de la multicolinealidad de las variables? ¿Es esto consistente con el resultado obtenido en la pregunta anterior?

```{r fig.align='center', fig.width=5, fig.height=1, echo=FALSE}

```




